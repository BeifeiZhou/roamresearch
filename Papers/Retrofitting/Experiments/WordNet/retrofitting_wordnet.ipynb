{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrofitting to the Structure of Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'Ben Lengerich'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from nltk.corpus import wordnet as wn\n",
    "import time\n",
    "\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../../'))\n",
    "from retrofit_identity import retrofit_identity\n",
    "from retrofit_linear   import retrofit_linear\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_filename='../../GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_filename, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize WordNet Representations as Pre-Trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 115635 WordNet lemmas in Word2Vec, ignored 91343 WordNet lemmas.\n",
      "Took 2.40 seconds.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "lemmas = []\n",
    "embeddings = []\n",
    "row_names = []\n",
    "row_ids = {}\n",
    "row_ids_by_name = {}\n",
    "n_ignored = 0\n",
    "for synset in wn.all_synsets():\n",
    "    for lemma in synset.lemmas():\n",
    "        if lemma.name() in row_ids:\n",
    "            continue\n",
    "        try:\n",
    "            embeddings.append(model[lemma.name()])\n",
    "            lemmas.append(lemma)\n",
    "            row_ids[str(lemma)] = len(embeddings) - 1\n",
    "            row_ids_by_name[lemma.name()] = len(embeddings) - 1\n",
    "            row_names.append(str(lemma))\n",
    "        except KeyError: # word present in WordNet, but not Word2Vec\n",
    "            n_ignored += 1\n",
    "            pass\n",
    "\n",
    "X = np.array(embeddings)\n",
    "row_names = np.array(row_names)\n",
    "save_obj(row_names, 'row_names')\n",
    "save_obj(row_ids,   'row_ids')\n",
    "assert(len(lemmas) == len(X))\n",
    "print(\"Found {} WordNet lemmas in Word2Vec, ignored {} WordNet lemmas.\".format(\n",
    "    len(X), n_ignored))\n",
    "print(\"Took {:.2f} seconds.\".format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced to 25 components.\n",
      "Took 8.46 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Reduce Embedding Dimensionality\n",
    "t = time.time()\n",
    "n_components=25\n",
    "pca = IncrementalPCA(n_components=n_components)\n",
    "X = pca.fit_transform(X)\n",
    "save_obj(X, \"X_reduced_{:d}\".format(n_components))\n",
    "print(\"Reduced to {:d} components.\\nTook {:.2f} seconds.\".format(n_components, time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extract Edges from WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 Edges of Type: Usage Domains\n",
      "60250 Edges of Type: Derivationally Related Forms\n",
      "136235 Edges of Type: Hypernyms\n",
      "5573 Edges of Type: Pertainyms\n",
      "136235 Edges of Type: Hyponyms\n",
      "5922 Edges of Type: Antonyms\n",
      "Took 5.90 seconds.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "t = time.time()\n",
    "def make_neighbors(get_neighbors, lemmas, row_ids):\n",
    "    assert(len(row_ids.keys()) == len(lemmas))\n",
    "    neighbors = {}\n",
    "    for x in lemmas:\n",
    "        x_row_id = row_ids[str(x)]\n",
    "        neighbors[x_row_id] = set([])\n",
    "        for y in get_neighbors(x):\n",
    "            try:\n",
    "                y_row_id = row_ids[str(y)]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            neighbors[x_row_id].add(y_row_id)\n",
    "        neighbors[x_row_id] = list(neighbors[x_row_id])\n",
    "    return neighbors\n",
    "\n",
    "hypernyms = lambda x: [l for s in x.synset().hypernyms() for l in s.lemmas()]\n",
    "hyponyms = lambda x: [l for s in x.synset().hyponyms() for l in s.lemmas()]\n",
    "antonyms = lambda x: x.antonyms()\n",
    "derivation = lambda x: x.derivationally_related_forms()\n",
    "also_sees = lambda x: x.also_sees()\n",
    "verb_groups = lambda x: x.verb_groups()\n",
    "pertainyms = lambda x: x.pertainyms()\n",
    "topic_domains = lambda x: x.topic_domains()\n",
    "usage_domains = lambda x: x.usage_domains()\n",
    "region_domains = lambda x: x.region_domains()\n",
    "\n",
    "edges = {'Hypernyms': make_neighbors(hypernyms, lemmas, row_ids)\n",
    "         ,'Hyponyms': make_neighbors(hyponyms, lemmas, row_ids)\n",
    "         ,'Antonyms': make_neighbors(antonyms, lemmas, row_ids)\n",
    "         ,'Derivationally Related Forms': make_neighbors(derivation, lemmas, row_ids)\n",
    "         ,'Usage Domains': make_neighbors(usage_domains, lemmas, row_ids)\n",
    "         ,'Also Sees': make_neighbors(also_sees, lemmas, row_ids)\n",
    "         ,'Verb Groups': make_neighbors(verb_groups, lemmas, row_ids)\n",
    "         ,'Pertainyms': make_neighbors(pertainyms, lemmas, row_ids)\n",
    "         ,'Topic Domains': make_neighbors(topic_domains, lemmas, row_ids)\n",
    "         ,'Region Domains': make_neighbors(region_domains, lemmas, row_ids)\n",
    "        }\n",
    "\n",
    "min_threshold = 15\n",
    "edges_parsed = {}\n",
    "for r in edges.keys():\n",
    "    if sum([len(neighbors) for neighbors in edges[r].values()]) > min_threshold:\n",
    "        edges_parsed[r] = edges[r]\n",
    "\n",
    "bad = 0\n",
    "def make_out_r(in_edges):\n",
    "    out_edges = {}\n",
    "    for i, neighbors in in_edges.items():\n",
    "        for j in neighbors:\n",
    "            try:\n",
    "                out_edges[j].append(i)\n",
    "            except KeyError:\n",
    "                out_edges[j] = [i]\n",
    "        if i not in out_edges:\n",
    "            out_edges[i] = []\n",
    "    return out_edges\n",
    "out_edges = {r: make_out_r(e) for r, e in edges_parsed.items()}\n",
    "\n",
    "save_obj(edges_parsed, \"in_edges\")\n",
    "save_obj(out_edges, \"out_edges\")\n",
    "print_edge_counts(edges_parsed)\n",
    "print(\"Took {:.2f} seconds.\".format(time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrofit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_obj(\"X_reduced_{:d}\".format(n_components))\n",
    "in_edges = load_obj(\"in_edges\")\n",
    "out_edges = load_obj(\"out_edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 6\n",
      "Baseline retrofitting took 9.42 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Retrofit Identity (Faruqui et al)\n",
    "t = time.time()\n",
    "retrofitted_baseline = retrofit_identity(\n",
    "    X, in_edges, n_iter=20, alpha=lambda i: 1, verbose=True)\n",
    "print(\"Baseline retrofitting took {:.2f} seconds.\".format(time.time() - t))\n",
    "save_obj(retrofitted_baseline, 'retrofitted_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 20\tChanges: 0.632\tLoss: 133914.809\n",
      "Iteration 2 of 20\tChanges: 0.579\tLoss: 131307.219\n",
      "Iteration 3 of 20\tChanges: 0.528\tLoss: 129638.506\n",
      "Iteration 4 of 20\tChanges: 0.476\tLoss: 128538.368\n",
      "Iteration 5 of 20\tChanges: 0.427\tLoss: 127789.000\n",
      "Iteration 6 of 20\tChanges: 0.381\tLoss: 127260.309\n",
      "Iteration 7 of 20\tChanges: 0.339\tLoss: 126873.428\n",
      "Iteration 8 of 20\tChanges: 0.301\tLoss: 126579.837\n",
      "Iteration 9 of 20\tChanges: 0.267\tLoss: 126349.244\n",
      "Iteration 10 of 20\tChanges: 0.238\tLoss: 126162.433\n",
      "Iteration 11 of 20\tChanges: 0.211\tLoss: 126007.008\n",
      "Iteration 12 of 20\tChanges: 0.188\tLoss: 125874.821\n",
      "Iteration 13 of 20\tChanges: 0.168\tLoss: 125760.400\n",
      "Iteration 14 of 20\tChanges: 0.150\tLoss: 125659.985\n",
      "Iteration 15 of 20\tChanges: 0.134\tLoss: 125570.919\n",
      "Iteration 16 of 20\tChanges: 0.120\tLoss: 125491.275\n",
      "Iteration 17 of 20\tChanges: 0.108\tLoss: 125419.611\n",
      "Iteration 18 of 20\tChanges: 0.097\tLoss: 125354.818\n",
      "Iteration 19 of 20\tChanges: 0.088\tLoss: 125296.017\n",
      "Iteration 20 of 20\tChanges: 0.080\tLoss: 125242.496\n",
      "Stopping at iteration 20; change was 0.080\n",
      "Linear retrofitting took 2214.17 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Linear\n",
    "t = time.time()\n",
    "retrofitted_linear, A, B = retrofit_linear(X, in_edges, out_edges, n_iter=20,\n",
    "                                           alpha=lambda i: 1, orthogonal=True,\n",
    "                                           lam=1e-3, lr=0.1, lr_decay=0.95, verbose=True)\n",
    "print(\"Linear retrofitting took {:.2f} seconds.\".format(time.time() - t))\n",
    "retrofitted_linear = np.squeeze(retrofitted_linear)\n",
    "assert(retrofitted_baseline.shape == retrofitted_linear.shape)\n",
    "save_obj(retrofitted_linear, 'retrofitted_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "These evaluation codes are adapted from Chris Potts's [CS244u course notes](http://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/master/vsm.ipynb#In-class-bake-off:-Word-similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "vsmdata_home = \"Evaluation/vsmdata\"\n",
    "\n",
    "def wordsim_dataset_reader(src_filename, header=False, delimiter=','):    \n",
    "    \"\"\"Basic reader that works for all four files, since they all have the \n",
    "    format word1,word2,score, differing only in whether or not they include \n",
    "    a header line and what delimiter they use.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename : str\n",
    "        Full path to the source file.\n",
    "        \n",
    "    header : bool (default: False)\n",
    "        Whether `src_filename` has a header.\n",
    "        \n",
    "    delimiter : str (default: ',')\n",
    "        Field delimiter in `src_filename`.\n",
    "    \n",
    "    Yields\n",
    "    ------    \n",
    "    (str, str, float)\n",
    "       (w1, w2, score) where `score` is the negative of the similarity \n",
    "       score in the file so that we are intuitively aligned with our \n",
    "       distance-based code.\n",
    "    \n",
    "    \"\"\"\n",
    "    reader = csv.reader(open(src_filename), delimiter=delimiter)\n",
    "    if header:\n",
    "        next(reader)\n",
    "    for row in reader:\n",
    "        w1, w2, score = row\n",
    "        # Negative of scores to align intuitively with distance functions:\n",
    "        score = -float(score)\n",
    "        yield (w1, w2, score)\n",
    "\n",
    "def wordsim353_reader():\n",
    "    \"\"\"WordSim-353: http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\"\"\"\n",
    "    src_filename = os.path.join(vsmdata_home, 'wordsim', 'wordsim353.csv')\n",
    "    return wordsim_dataset_reader(src_filename, header=True)\n",
    " \n",
    "def mturk287_reader():\n",
    "    \"\"\"MTurk-287: http://tx.technion.ac.il/~kirar/Datasets.html\"\"\"\n",
    "    src_filename = os.path.join(vsmdata_home, 'wordsim', 'MTurk-287.csv')\n",
    "    return wordsim_dataset_reader(src_filename, header=False)\n",
    "    \n",
    "def mturk771_reader():\n",
    "    \"\"\"MTURK-771: http://www2.mta.ac.il/~gideon/mturk771.html\"\"\"\n",
    "    src_filename = os.path.join(vsmdata_home, 'wordsim', 'MTURK-771.csv')\n",
    "    return wordsim_dataset_reader(src_filename, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):        \n",
    "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
    "    the same dimensionality. Returns a float.\"\"\"\n",
    "    # Use scipy's method:\n",
    "    return scipy.spatial.distance.cosine(u, v)\n",
    "    # Or define it yourself:\n",
    "    # return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))\n",
    "    \n",
    "def word_similarity_evaluation(reader, mat, row_ids, distfunc=cosine):\n",
    "    \"\"\"Word-similarity evalution framework.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    reader : iterator\n",
    "        A reader for a word-similarity dataset. Just has to yield\n",
    "        tuples (word1, word2, score).\n",
    "    \n",
    "    mat : 2d np.array\n",
    "        The VSM being evaluated.\n",
    "        \n",
    "    rownames : dict\n",
    "        The names of the rows in mat.\n",
    "        \n",
    "    distfunc : function mapping vector pairs to floats (default: `cosine`)\n",
    "        The measure of distance between vectors. Can also be `euclidean`, \n",
    "        `matching`, `jaccard`, as well as any other distance measure \n",
    "        between 1d vectors.  \n",
    "    \n",
    "    Prints\n",
    "    ------\n",
    "    To standard output\n",
    "        Size of the vocabulary overlap between the evaluation set and\n",
    "        rownames. We limit the evalation to the overlap, paying no price\n",
    "        for missing words (which is not fair, but it's reasonable given\n",
    "        that we're working with very small VSMs in this notebook).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The Spearman rank correlation coefficient between the dataset\n",
    "        scores and the similarity values obtained from `mat` using \n",
    "        `distfunc`. This evaluation is sensitive only to rankings, not\n",
    "        to absolute values.\n",
    "    \n",
    "    \"\"\"    \n",
    "    sims = defaultdict(list)\n",
    "    vocab = set([])\n",
    "    for w1, w2, score in reader():\n",
    "        if w1 in row_ids and w2 in row_ids:\n",
    "            sims[w1].append((w2, score))\n",
    "            sims[w2].append((w1, score))\n",
    "            vocab.add(w1)\n",
    "            vocab.add(w2)\n",
    "    print(\"Evaluation vocabulary size: %s\" % len(vocab))\n",
    "    # Evaluate the matrix by creating a vector of all_scores for data\n",
    "    # and all_dists for mat's distances. \n",
    "    all_scores = []\n",
    "    all_dists = []\n",
    "    for word in vocab:\n",
    "        vec = mat[row_ids[word]]\n",
    "        vals = sims[word]\n",
    "        cmps, scores = zip(*vals)\n",
    "        all_scores += scores\n",
    "        all_dists += [distfunc(vec, mat[row_ids[w]]) for w in cmps]\n",
    "    # Return just the rank correlation coefficient (index [1] would be the p-value):\n",
    "    return scipy.stats.spearmanr(all_scores, all_dists)[0]\n",
    "\n",
    "def full_word_similarity_evaluation(mat, row_ids):\n",
    "    \"\"\"Evaluate the (mat, rownames) VSM against all four datasets.\"\"\"\n",
    "    for reader in (wordsim353_reader, mturk771_reader, mturk287_reader):\n",
    "        print(\"-\"*40)\n",
    "        print(reader.__name__)\n",
    "        print('Spearman r: %0.03f' % word_similarity_evaluation(reader, mat, row_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributional Embeddings\n",
      "----------------------------------------\n",
      "wordsim353_reader\n",
      "Evaluation vocabulary size: 429\n",
      "Spearman r: 0.505\n",
      "----------------------------------------\n",
      "mturk771_reader\n",
      "Evaluation vocabulary size: 1109\n",
      "Spearman r: 0.535\n",
      "----------------------------------------\n",
      "mturk287_reader\n",
      "Evaluation vocabulary size: 301\n",
      "Spearman r: 0.652\n",
      "================================================================================\n",
      "Distributional Embeddings, Identity Retrofitting\n",
      "----------------------------------------\n",
      "wordsim353_reader\n",
      "Evaluation vocabulary size: 429\n",
      "Spearman r: 0.524\n",
      "----------------------------------------\n",
      "mturk771_reader\n",
      "Evaluation vocabulary size: 1109\n",
      "Spearman r: 0.511\n",
      "----------------------------------------\n",
      "mturk287_reader\n",
      "Evaluation vocabulary size: 301\n",
      "Spearman r: 0.627\n",
      "================================================================================\n",
      "Distributional Embeddings, Linear Retrofitting\n",
      "----------------------------------------\n",
      "wordsim353_reader\n",
      "Evaluation vocabulary size: 429\n",
      "Spearman r: 0.531\n",
      "----------------------------------------\n",
      "mturk771_reader\n",
      "Evaluation vocabulary size: 1109\n",
      "Spearman r: 0.539\n",
      "----------------------------------------\n",
      "mturk287_reader\n",
      "Evaluation vocabulary size: 301\n",
      "Spearman r: 0.670\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "X = load_obj(\"X_reduced_{:d}\".format(n_components))\n",
    "retrofitted_baseline = load_obj('retrofitted_baseline')\n",
    "retrofitted_linear = load_obj('retrofitted_linear')\n",
    "\n",
    "print(\"Distributional Embeddings\")\n",
    "full_word_similarity_evaluation(X, row_ids_by_name)\n",
    "print(\"=\"*80)\n",
    "print(\"Distributional Embeddings, Identity Retrofitting\")\n",
    "full_word_similarity_evaluation(retrofitted_baseline, row_ids_by_name)\n",
    "print(\"=\"*80)\n",
    "print(\"Distributional Embeddings, Linear Retrofitting\")\n",
    "full_word_similarity_evaluation(retrofitted_linear, row_ids_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing Syntatic Relations for Word2Vec Distributional Embeddings\n",
      "Tested 12148 Analogies, Mean Cosine Similarity: 0.76070\n",
      "============================================================\n",
      "Testing Syntatic Relations for Word2Vec, Identity Retrofitting\n",
      "Tested 12148 Analogies, Mean Cosine Similarity: 0.75057\n",
      "============================================================\n",
      "Testing Syntatic Relations for Word2Vec, Linear Retrofitting\n",
      "Tested 12148 Analogies, Mean Cosine Similarity: 0.77100\n"
     ]
    }
   ],
   "source": [
    "def load_synrel(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = [p.strip() for p in line.split(' ')]\n",
    "            if len(parts) == 4:\n",
    "                data.append(parts)\n",
    "    return data\n",
    "\n",
    "def cos(a, b):\n",
    "    return a.T.dot(b)/(np.linalg.norm(a, ord=2)*np.linalg.norm(b, ord=2))\n",
    "\n",
    "def syn_rel_evaluation(X, row_ids):\n",
    "    data = load_synrel('Evaluation/synreldata/analogies.txt')\n",
    "    sims = []\n",
    "    for analogy in data:\n",
    "        try:\n",
    "            sims.append(\n",
    "                cos(X[row_ids[analogy[1]]] - X[row_ids[analogy[0]]] + X[row_ids[analogy[2]]],\n",
    "                    X[row_ids[analogy[3]]]))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    print(\"Tested {:d} Analogies, Mean Cosine Similarity: {:.5f}\".format(len(sims), np.mean(np.array(sims))))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Syntatic Relations for Word2Vec Distributional Embeddings\")\n",
    "syn_rel_evaluation(X, row_ids_by_name)\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Syntatic Relations for Word2Vec, Identity Retrofitting\")\n",
    "syn_rel_evaluation(retrofitted_baseline, row_ids_by_name)\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Syntatic Relations for Word2Vec, Linear Retrofitting\")\n",
    "syn_rel_evaluation(retrofitted_linear, row_ids_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "navigate_menu": false,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
