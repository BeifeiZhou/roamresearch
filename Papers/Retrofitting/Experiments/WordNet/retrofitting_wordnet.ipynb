{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrofitting to the Structure of Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'Ben Lengerich'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from nltk.corpus import wordnet as wn\n",
    "import time\n",
    "\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../../'))\n",
    "from retrofit_identity import retrofit_identity\n",
    "from retrofit_linear   import retrofit_linear\n",
    "from retrofit_neural   import retrofit_neural\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_filename='../../GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_filename, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize WordNet Representations as Pre-Trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 115635 WordNet lemmas in Word2Vec, ignored 91343 WordNet lemmas.\n",
      "Took 2.27 seconds.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "lemmas = []\n",
    "embeddings = []\n",
    "row_names = []\n",
    "row_ids = {}\n",
    "row_ids_by_name = {}\n",
    "n_ignored = 0\n",
    "for synset in wn.all_synsets():\n",
    "    for lemma in synset.lemmas():\n",
    "        if lemma.name() in row_ids:\n",
    "            continue\n",
    "        try:\n",
    "            embeddings.append(model[lemma.name()])\n",
    "            lemmas.append(lemma)\n",
    "            row_ids[str(lemma)] = len(embeddings) - 1\n",
    "            row_ids_by_name[lemma.name()] = len(embeddings) - 1\n",
    "            row_names.append(str(lemma))\n",
    "        except KeyError: # word present in WordNet, but not Word2Vec\n",
    "            n_ignored += 1\n",
    "            pass\n",
    "\n",
    "X = np.array(embeddings)\n",
    "row_names = np.array(row_names)\n",
    "save_obj(row_names, 'row_names')\n",
    "save_obj(row_ids,   'row_ids')\n",
    "save_obj(row_ids_by_name, 'row_ids_by_name')\n",
    "assert(len(lemmas) == len(X))\n",
    "print(\"Found {} WordNet lemmas in Word2Vec, ignored {} WordNet lemmas.\".format(\n",
    "    len(X), n_ignored))\n",
    "print(\"Took {:.2f} seconds.\".format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced to 25 components.\n",
      "Took 8.34 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Reduce Embedding Dimensionality\n",
    "t = time.time()\n",
    "n_components=25\n",
    "pca = IncrementalPCA(n_components=n_components)\n",
    "X = pca.fit_transform(X)\n",
    "save_obj(X, \"X_reduced_{:d}\".format(n_components))\n",
    "print(\"Reduced to {:d} components.\\nTook {:.2f} seconds.\".format(n_components, time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extract Edges from WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5573 Edges of Type: Pertainyms\n",
      "136235 Edges of Type: Hyponyms\n",
      "69 Edges of Type: Usage Domains\n",
      "60250 Edges of Type: Derivationally Related Forms\n",
      "136235 Edges of Type: Hypernyms\n",
      "5922 Edges of Type: Antonyms\n",
      "Took 6.28 seconds.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "t = time.time()\n",
    "def make_neighbors(get_neighbors, lemmas, row_ids):\n",
    "    assert(len(row_ids.keys()) == len(lemmas))\n",
    "    neighbors = {}\n",
    "    for x in lemmas:\n",
    "        x_row_id = row_ids[str(x)]\n",
    "        neighbors[x_row_id] = set([])\n",
    "        for y in get_neighbors(x):\n",
    "            try:\n",
    "                y_row_id = row_ids[str(y)]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            neighbors[x_row_id].add(y_row_id)\n",
    "        neighbors[x_row_id] = list(neighbors[x_row_id])\n",
    "    return neighbors\n",
    "\n",
    "hypernyms = lambda x: [l for s in x.synset().hypernyms() for l in s.lemmas()]\n",
    "hyponyms = lambda x: [l for s in x.synset().hyponyms() for l in s.lemmas()]\n",
    "antonyms = lambda x: x.antonyms()\n",
    "derivation = lambda x: x.derivationally_related_forms()\n",
    "also_sees = lambda x: x.also_sees()\n",
    "verb_groups = lambda x: x.verb_groups()\n",
    "pertainyms = lambda x: x.pertainyms()\n",
    "topic_domains = lambda x: x.topic_domains()\n",
    "usage_domains = lambda x: x.usage_domains()\n",
    "region_domains = lambda x: x.region_domains()\n",
    "\n",
    "edges = {'Hypernyms': make_neighbors(hypernyms, lemmas, row_ids)\n",
    "         ,'Hyponyms': make_neighbors(hyponyms, lemmas, row_ids)\n",
    "         ,'Antonyms': make_neighbors(antonyms, lemmas, row_ids)\n",
    "         ,'Derivationally Related Forms': make_neighbors(derivation, lemmas, row_ids)\n",
    "         ,'Usage Domains': make_neighbors(usage_domains, lemmas, row_ids)\n",
    "         ,'Also Sees': make_neighbors(also_sees, lemmas, row_ids)\n",
    "         ,'Verb Groups': make_neighbors(verb_groups, lemmas, row_ids)\n",
    "         ,'Pertainyms': make_neighbors(pertainyms, lemmas, row_ids)\n",
    "         ,'Topic Domains': make_neighbors(topic_domains, lemmas, row_ids)\n",
    "         ,'Region Domains': make_neighbors(region_domains, lemmas, row_ids)\n",
    "        }\n",
    "\n",
    "min_threshold = 15\n",
    "edges_parsed = {}\n",
    "for r in edges.keys():\n",
    "    if sum([len(neighbors) for neighbors in edges[r].values()]) > min_threshold:\n",
    "        edges_parsed[r] = edges[r]\n",
    "\n",
    "bad = 0\n",
    "def make_out_r(in_edges):\n",
    "    out_edges = {}\n",
    "    for i, neighbors in in_edges.items():\n",
    "        for j in neighbors:\n",
    "            try:\n",
    "                out_edges[j].append(i)\n",
    "            except KeyError:\n",
    "                out_edges[j] = [i]\n",
    "        if i not in out_edges:\n",
    "            out_edges[i] = []\n",
    "    return out_edges\n",
    "out_edges = {r: make_out_r(e) for r, e in edges_parsed.items()}\n",
    "\n",
    "save_obj(edges_parsed, \"in_edges\")\n",
    "save_obj(out_edges, \"out_edges\")\n",
    "print_edge_counts(edges_parsed)\n",
    "print(\"Took {:.2f} seconds.\".format(time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrofit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components=25\n",
    "X = load_obj(\"X_reduced_{:d}\".format(n_components))\n",
    "in_edges = load_obj(\"in_edges\")\n",
    "out_edges = load_obj(\"out_edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 6\n",
      "Baseline retrofitting took 8.21 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Retrofit with Identity Relation (Faruqui et al)\n",
    "t = time.time()\n",
    "retrofitted_identity = retrofit_identity(\n",
    "    X, in_edges, n_iter=20, alpha=lambda i: 1, verbose=True)\n",
    "print(\"Baseline retrofitting took {:.2f} seconds.\".format(time.time() - t))\n",
    "save_obj(retrofitted_identity, 'retrofitted_identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 50\tChanges: 0.554\tLoss: 134913.381\n",
      "Iteration 2 of 50\tChanges: 0.517\tLoss: 132132.389\n",
      "Iteration 3 of 50\tChanges: 0.482\tLoss: 130346.684\n",
      "Iteration 4 of 50\tChanges: 0.448\tLoss: 129204.179\n",
      "Iteration 5 of 50\tChanges: 0.414\tLoss: 128474.393\n",
      "Iteration 6 of 50\tChanges: 0.380\tLoss: 128007.934\n",
      "Iteration 7 of 50\tChanges: 0.346\tLoss: 127708.801\n",
      "Iteration 8 of 50\tChanges: 0.314\tLoss: 127515.815\n",
      "Iteration 9 of 50\tChanges: 0.284\tLoss: 127390.308\n",
      "Iteration 10 of 50\tChanges: 0.256\tLoss: 127308.038\n",
      "Iteration 11 of 50\tChanges: 0.230\tLoss: 127253.922\n",
      "Iteration 12 of 50\tChanges: 0.205\tLoss: 127218.629\n",
      "Iteration 13 of 50\tChanges: 0.183\tLoss: 127196.397\n",
      "Iteration 14 of 50\tChanges: 0.163\tLoss: 127183.643\n",
      "Iteration 15 of 50\tChanges: 0.145\tLoss: 127178.095\n",
      "Loss reached local minimum at iteration 15\n",
      "Linear retrofitting took 1706.63 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Retrofit with Linear Relation\n",
    "t = time.time()\n",
    "retrofitted_linear, A, B = retrofit_linear(X, in_edges, out_edges, n_iter=50,\n",
    "                                           alpha=lambda i: 1, orthogonal=True,\n",
    "                                           lam=1e-3, lr=0.1, lr_decay=0.99, verbose=True)\n",
    "print(\"Linear retrofitting took {:.2f} seconds.\".format(time.time() - t))\n",
    "retrofitted_linear = np.squeeze(retrofitted_linear)\n",
    "assert(retrofitted_identity.shape == retrofitted_linear.shape)\n",
    "save_obj(retrofitted_linear, 'retrofitted_linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5 of 2000\tChanges: 1.26373\tLoss: 1581.857\tPatience: 99\n",
      "Iteration 10 of 2000\tChanges: 0.23948\tLoss: 1174.324\tPatience: 98\n",
      "Iteration 15 of 2000\tChanges: 0.24093\tLoss: 999.598\tPatience: 96\n",
      "Iteration 20 of 2000\tChanges: 0.11356\tLoss: 724.107\tPatience: 93\n",
      "Iteration 25 of 2000\tChanges: 0.13395\tLoss: 712.741\tPatience: 91\n",
      "Iteration 30 of 2000\tChanges: 0.06782\tLoss: 513.152\tPatience: 89\n",
      "Iteration 35 of 2000\tChanges: 0.08978\tLoss: 494.278\tPatience: 88\n",
      "Iteration 40 of 2000\tChanges: 0.09615\tLoss: 439.267\tPatience: 85\n",
      "Iteration 45 of 2000\tChanges: 0.07010\tLoss: 445.482\tPatience: 82\n",
      "Iteration 50 of 2000\tChanges: 0.05834\tLoss: 427.946\tPatience: 79\n",
      "Iteration 55 of 2000\tChanges: 0.05734\tLoss: 339.042\tPatience: 78\n",
      "Iteration 60 of 2000\tChanges: 0.10240\tLoss: 267.613\tPatience: 75\n",
      "Iteration 65 of 2000\tChanges: 0.04350\tLoss: 253.538\tPatience: 73\n",
      "Iteration 70 of 2000\tChanges: 0.08645\tLoss: 364.526\tPatience: 70\n",
      "Iteration 75 of 2000\tChanges: 0.05637\tLoss: 348.364\tPatience: 67\n",
      "Iteration 80 of 2000\tChanges: 0.10063\tLoss: 353.687\tPatience: 65\n",
      "Iteration 85 of 2000\tChanges: 0.03620\tLoss: 271.617\tPatience: 62\n",
      "Iteration 90 of 2000\tChanges: 0.05127\tLoss: 308.999\tPatience: 58\n",
      "Iteration 95 of 2000\tChanges: 0.04150\tLoss: 272.046\tPatience: 55\n",
      "Iteration 100 of 2000\tChanges: 0.05521\tLoss: 331.137\tPatience: 52\n",
      "Iteration 105 of 2000\tChanges: 0.03482\tLoss: 452.397\tPatience: 49\n",
      "Iteration 110 of 2000\tChanges: 0.02829\tLoss: 347.083\tPatience: 47\n",
      "Iteration 115 of 2000\tChanges: 0.02509\tLoss: 310.122\tPatience: 45\n",
      "Iteration 120 of 2000\tChanges: 0.03046\tLoss: 272.981\tPatience: 43\n",
      "Iteration 125 of 2000\tChanges: 0.03985\tLoss: 274.096\tPatience: 41\n",
      "Iteration 130 of 2000\tChanges: 0.01699\tLoss: 321.116\tPatience: 38\n",
      "Iteration 135 of 2000\tChanges: 0.01582\tLoss: 260.284\tPatience: 35\n",
      "Iteration 140 of 2000\tChanges: 0.03762\tLoss: 288.945\tPatience: 33\n",
      "Iteration 145 of 2000\tChanges: 0.01789\tLoss: 272.389\tPatience: 30\n",
      "Iteration 150 of 2000\tChanges: 0.02212\tLoss: 253.345\tPatience: 27\n",
      "Iteration 155 of 2000\tChanges: 0.01438\tLoss: 217.376\tPatience: 25\n",
      "Iteration 160 of 2000\tChanges: 0.01315\tLoss: 203.496\tPatience: 23\n",
      "Iteration 165 of 2000\tChanges: 0.01251\tLoss: 227.375\tPatience: 20\n",
      "Iteration 170 of 2000\tChanges: 0.01259\tLoss: 218.808\tPatience: 18\n",
      "Iteration 175 of 2000\tChanges: 0.02110\tLoss: 186.024\tPatience: 16\n",
      "Iteration 180 of 2000\tChanges: 0.01653\tLoss: 260.670\tPatience: 14\n",
      "Iteration 185 of 2000\tChanges: 0.01364\tLoss: 179.882\tPatience: 13\n",
      "Iteration 190 of 2000\tChanges: 0.00939\tLoss: 199.458\tPatience: 11\n",
      "Iteration 195 of 2000\tChanges: 0.01098\tLoss: 249.809\tPatience: 8\n",
      "Iteration 200 of 2000\tChanges: 0.01226\tLoss: 241.571\tPatience: 6\n",
      "Iteration 205 of 2000\tChanges: 0.00786\tLoss: 243.432\tPatience: 4\n",
      "Iteration 210 of 2000\tChanges: 0.01170\tLoss: 251.946\tPatience: 2\n",
      "Loss reached local minimum (and patience expired) at iteration 214\n",
      "Neural retrofitting took 246.50 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Neural Retrofitting\n",
    "t = time.time()\n",
    "retrofitted_neural, A, u, b = retrofit_neural(\n",
    "    X, in_edges, out_edges, k=10, alpha=lambda i: 1,\n",
    "    n_iter=2000, lam=1e-5, lr=0.001, tol=1e-5, lr_decay=0.99, batch_size=256, patience=100, verbose=5)\n",
    "print(\"Neural retrofitting took {:.2f} seconds.\".format(time.time() - t))\n",
    "save_obj(retrofitted_neural, 'retrofitted_neural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The code for evaluating word similarity and syntatic relations are adapted from Chris Potts's [CS244u course notes](http://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/master/vsm.ipynb#In-class-bake-off:-Word-similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributional Embeddings\n",
      "----------------------------------------\n",
      "wordsim353_reader\n",
      "Evaluation vocabulary size: 429\n",
      "Spearman r: 0.512\n",
      "----------------------------------------\n",
      "mturk771_reader\n",
      "Evaluation vocabulary size: 1109\n",
      "Spearman r: 0.538\n",
      "----------------------------------------\n",
      "mturk287_reader\n",
      "Evaluation vocabulary size: 301\n",
      "Spearman r: 0.671\n",
      "================================================================================\n",
      "Distributional Embeddings, Identity Retrofitting\n",
      "----------------------------------------\n",
      "wordsim353_reader\n",
      "Evaluation vocabulary size: 429\n",
      "Spearman r: 0.512\n",
      "----------------------------------------\n",
      "mturk771_reader\n",
      "Evaluation vocabulary size: 1109\n",
      "Spearman r: 0.532\n",
      "----------------------------------------\n",
      "mturk287_reader\n",
      "Evaluation vocabulary size: 301\n",
      "Spearman r: 0.664\n",
      "================================================================================\n",
      "Distributional Embeddings, Linear Retrofitting\n",
      "----------------------------------------\n",
      "wordsim353_reader\n",
      "Evaluation vocabulary size: 429\n",
      "Spearman r: 0.542\n",
      "----------------------------------------\n",
      "mturk771_reader\n",
      "Evaluation vocabulary size: 1109\n",
      "Spearman r: 0.562\n",
      "----------------------------------------\n",
      "mturk287_reader\n",
      "Evaluation vocabulary size: 301\n",
      "Spearman r: 0.679\n",
      "================================================================================\n",
      "Distributional Embeddings, Neural Retrofitting\n",
      "----------------------------------------\n",
      "wordsim353_reader\n",
      "Evaluation vocabulary size: 429\n",
      "Spearman r: 0.512\n",
      "----------------------------------------\n",
      "mturk771_reader\n",
      "Evaluation vocabulary size: 1109\n",
      "Spearman r: 0.538\n",
      "----------------------------------------\n",
      "mturk287_reader\n",
      "Evaluation vocabulary size: 301\n",
      "Spearman r: 0.672\n"
     ]
    }
   ],
   "source": [
    "from evaluate import *\n",
    "n_components = 25\n",
    "X = load_obj(\"X_reduced_{:d}\".format(n_components))\n",
    "row_ids_by_name = load_obj('row_ids_by_name')\n",
    "retrofitted_identity = load_obj('retrofitted_identity')\n",
    "retrofitted_linear = load_obj('retrofitted_linear')\n",
    "retrofitted_neural = load_obj('retrofitted_neural')\n",
    "\n",
    "print(\"Distributional Embeddings\")\n",
    "full_word_similarity_evaluation(X, row_ids_by_name)\n",
    "print(\"=\"*80)\n",
    "print(\"Distributional Embeddings, Identity Retrofitting\")\n",
    "full_word_similarity_evaluation(retrofitted_identity, row_ids_by_name)\n",
    "print(\"=\"*80)\n",
    "print(\"Distributional Embeddings, Linear Retrofitting\")\n",
    "full_word_similarity_evaluation(retrofitted_linear, row_ids_by_name)\n",
    "print(\"=\"*80)\n",
    "print(\"Distributional Embeddings, Neural Retrofitting\")\n",
    "full_word_similarity_evaluation(retrofitted_neural, row_ids_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Syntatic Relations for Word2Vec Distributional Embeddings\n",
      "Tested 12148 Analogies, Mean Cosine Similarity: 0.77165\n",
      "============================================================\n",
      "Testing Syntatic Relations for Word2Vec, Identity Retrofitting\n",
      "Tested 12148 Analogies, Mean Cosine Similarity: 0.77419\n",
      "============================================================\n",
      "Testing Syntatic Relations for Word2Vec, Linear Retrofitting\n",
      "Tested 12148 Analogies, Mean Cosine Similarity: 0.79329\n",
      "============================================================\n",
      "Testing Syntatic Relations for Word2Vec, Neural Retrofitting\n",
      "Tested 12148 Analogies, Mean Cosine Similarity: 0.77162\n"
     ]
    }
   ],
   "source": [
    "def load_synrel(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = [p.strip() for p in line.split(' ')]\n",
    "            if len(parts) == 4:\n",
    "                data.append(parts)\n",
    "    return data\n",
    "\n",
    "def cos(a, b):\n",
    "    return a.T.dot(b)/(np.linalg.norm(a, ord=2)*np.linalg.norm(b, ord=2))\n",
    "\n",
    "def syn_rel_evaluation(X, row_ids):\n",
    "    data = load_synrel('Evaluation/synreldata/analogies.txt')\n",
    "    sims = []\n",
    "    for analogy in data:\n",
    "        try:\n",
    "            sims.append(\n",
    "                cos(X[row_ids[analogy[1]]] - X[row_ids[analogy[0]]] + X[row_ids[analogy[2]]],\n",
    "                    X[row_ids[analogy[3]]]))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    print(\"Tested {:d} Analogies, Mean Cosine Similarity: {:.5f}\".format(len(sims), np.mean(np.array(sims))))\n",
    "\n",
    "print(\"Testing Syntatic Relations for Word2Vec Distributional Embeddings\")\n",
    "syn_rel_evaluation(X, row_ids_by_name)\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Syntatic Relations for Word2Vec, Identity Retrofitting\")\n",
    "syn_rel_evaluation(retrofitted_identity, row_ids_by_name)\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Syntatic Relations for Word2Vec, Linear Retrofitting\")\n",
    "syn_rel_evaluation(retrofitted_linear, row_ids_by_name)\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Syntatic Relations for Word2Vec, Neural Retrofitting\")\n",
    "syn_rel_evaluation(retrofitted_neural, row_ids_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "navigate_menu": false,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
